<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="MVPNet">
  <meta name="keywords" content="Active Perception, Optimal Viewpoint Prediction, Multimodal Perception, Robotic Grasping">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MVPNet</title>


  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link rel="icon" type="image/x-icon" href="./static/images/logo4.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script src="static/js/bulma-carousel.js"></script>
  <style>
    .results-carousel {
        width: 80%;       /* Set to your desired width */
        margin: 0 auto;   /* Center the carousel horizontally */
    }

  </style>
<style>
  .slider-container {
    position: relative;
    width: 100%;
    max-width:1000px;
    margin: auto;
    overflow: hidden;
  }
  
  .slider-wrapper {
    display: flex;
    transition: transform 0.3s ease-out;
  }
  
  .slider-item {
  width: 100%; /* 让每个 .slider-item 填满 .slider-wrapper */
  height: 100%; /* 让每个 .slider-item 填满 .slider-wrapper */
  display: flex; /* 使用 Flexbox 布局来居中内容 */
  justify-content: center; /* 水平居中内容 */
  align-items: center; /* 垂直居中内容 */
  flex-shrink: 0; /* 防止 flex 子元素缩小 */
}

/* 如果 .slider-item 包含图片 */
.slider-item img {
  width: 100%; /* 宽度填满 .slider-item */
  height: auto; /* 高度自适应以保持图片比例 */
  object-fit: cover; /* 覆盖整个区域，可能会剪裁某些部分 */
}
  
  button {
    position: absolute;
    top: 50%;
    transform: translateY(-50%);
    background-color: #333;
    color: white;
    border: none;
    padding: 10px 10px;
    cursor: pointer;
    z-index: 2;
  }
  
  .left-btn {
    left: 10px;
  }
  
  .right-btn {
    right: 10px;
  }
  
  </style>
</head>
<body>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title" style="font-size: 2.7rem;"><img src="./static/images/logo4.ico" alt="Logo" style="width:1em; height:1em; vertical-align:middle; margin: 0 0.2em;">A General One-Shot Multimodal Active Perception Framework for Robotic Manipulation: Learning to Predict Optimal Viewpoint</h1>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://nkrobotlab.github.io/MVPNet/">Deyun Qin</a><sup>1</sup>,</span>
              <a href="https://nkrobotlab.github.io/MVPNet/">Zezhi Liu</a><sup>1</sup>,</span>
              <a href="https://nkrobotlab.github.io/MVPNet/">Hanqian Luo</a><sup>1,2</sup>,</span>
              <a href="https://ai.nankai.edu.cn/info/1034/4844.htm">Xiao Liang</a><sup>1</sup>,</span>
              <a href="https://scholar.google.com/citations?user=SNyDfVoAAAAJ&hl=zh-CN&oi=ao">Yongchun Fang</a><sup>1</sup></span>
            </span>
          </div>
      
      <div class="is-size-5 publication-authors">
        <span class="author-block"><sup>1</sup>Nankai University, </span>
        <span class="author-block"><sup>2</sup>The Hong Kong Polytechnic University</span>
      </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://nkrobotlab.github.io/MVPNet/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://nkrobotlab.github.io/MVPNet/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="./static/videos/video.mp4"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://nkrobotlab.github.io/MVPNet/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Comming Soon)</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

  
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body" style="text-align: center;">
      <img id="teaser" src="./static/images/fig_human_viewpoint_optimize.png" alt="Teaser Image" style="width: 65%; display: block; margin: 0 auto;">
      <h2 class="subtitle has-text-centered">
        The <strong>"Focus-then-Execute"</strong> active perception paradigm.
      </h2>
    </div>
  </div>
</section>
  

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-2" style="text-align: center;">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Active perception in vision-based robotic manipulation aims to move the camera toward more informative observation viewpoints, thereby providing high-quality perceptual inputs for downstream tasks.
            Most existing active perception methods rely on iterative optimization, leading to high time and motion costs, and are tightly coupled with task-specific objectives, which limits their transferability.
            In this paper, we propose a general one-shot multimodal active perception framework for robotic manipulation. The framework enables direct inference of optimal viewpoints and comprises a data collection pipeline and an optimal viewpoint prediction network.
            Specifically, the framework decouples viewpoint quality evaluation from the overall architecture, supporting heterogeneous task requirements. Optimal viewpoints are defined through systematic sampling and evaluation of candidate viewpoints, after which large-scale training datasets are constructed via domain randomization.
            Moreover, a multimodal optimal viewpoint prediction network is developed, leveraging cross-attention to align and fuse multimodal features and directly predict camera pose adjustments.
            The proposed framework is instantiated in robotic grasping under viewpoint-constrained environments. 
            Experimental results demonstrate that active perception guided by the framework significantly improves grasp success rates.
            Notably, real-world evaluations achieve nearly double the grasp success rate and enable seamless sim-to-real transfer without additional fine-tuning, demonstrating the effectiveness of the proposed framework.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

  
<section class="section">
  <div class="container is-max-desktop">
    <div class="content">
     <h2 class="title is-2" style="text-align: center;">Main Contribution</h2>
      <p>We propose <strong>a data-driven multimodal active perception framework </strong> that directly predicts the optimal observation viewpoint, enabling improved perception with <strong>only a single relook</strong>, and generalizes across different tasks.
       The main contributions of our work are summarized as follows:</p>
    </div>
    <!-- Explanation of the diagram -->
    <div class="content" style="margin-top: 20px;">
    <ul>
        <li>A general one-shot multimodal active perception framework is proposed, comprising a data collection pipeline and an optimal viewpoint prediction network. This framework enables the unified modeling of diverse task requirements, thereby extending its applicability to a broader range of task scenarios.</li>
        <li>An optimal observation viewpoint data collection pipeline is established, in which optimal viewpoints are defined through task-specific viewpoint quality evaluation functions, and large-scale datasets are constructed via domain randomization.</li>
        <li>An optimal observation viewpoint prediction network is developed. Utilizing the cross-attention mechanism, this network aligns and fuses multimodal features to predict the required camera pose adjustment.</li>
        <li>The proposed framework is instantiated in robotic grasping under viewpoint-constrained environments, where data collection and network training are conducted, and its effectiveness and robustness are validated through extensive simulation and real-world experiments.</li>
      </ul>
    </div>
    </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="content">
        <h2 class="title is-2" style="text-align: center;">Framework Overview</h2>
        <div style="text-align: center; margin-top: 20px;"> <!-- Center align the image container -->
          <img src="./static/images/fig_overall_framework.png"
              style="width: 100%; max-width: 1000px; height: auto; display: block; margin: 0 auto;" <!-- Make the image responsive and centered -->
        </div>
        <div style="margin-top: 5px;"></div>
        <p>Overall framework of the proposed method, illustrated with robotic grasping in viewpoint-constrained environments:
          (a) sampling and evaluating candidate viewpoints to obtain the optimal viewpoint for each object, followed by dataset construction via domain randomization;
          (b) training the MVPNet based on the constructed dataset; and
          (c) deploying the trained network and conducting comparative evaluations.
        </p>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="content">
        <h2 class="title is-2" style="text-align: center;">Network Architecture</h2>
        <div style="text-align: center; margin-top: 20px;"> <!-- Center align the image container -->
          <img src="./static/images/fig_network.png"
              style="width: 100%; max-width: 1000px; height: auto; display: block; margin: 0 auto;" <!-- Make the image responsive and centered -->
        </div>
        <div style="margin-top: 5px;"></div>
        <p>First, the current observation is obtained and preprocessed together with the natural language description of the target object. Subsequently, modality-specific encoders are employed to extract features, which are then aligned and fused using a Transformer. Finally, an MLP maps the fused representation to the camera pose adjustments.
        </p>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
          <h2  class="title is-2" style="text-align: center; padding-bottom: 10px;">Synthetic Dataset Construction</h2>
          <div class="row">
            <div class="col">
              <video autoplay muted loop playsinline controls src="./static/videos/video_define_optimal_observation_viewpoint.mp4" width="100%" style="border-radius:5px; "></video>
              <h3 class="title is-3" style="text-align: center; margin-bottom: 10px; font-size: x-large;"> Define Optimal Observation Viewpoint </h3>
            </div>
            <div class="col">
              <video autoplay muted loop playsinline controls src="./static/videos/video_data_collection.mp4" width="100%" style="border-radius:5px; "></video>
              <h3 class="title is-3" style="text-align: center; margin-bottom: 10px; font-size: x-large;"> Data Collection </h3>
            </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
          <h2  class="title is-2" style="text-align: center; padding-bottom: 10px;">Simulated Experiments</h2>
          <div class="row">
            <div class="col">
              <video autoplay muted loop playsinline controls src="./static/videos/video_simulated_experiments.mp4" width="100%" style="border-radius:5px; "></video>
            </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
          <h2  class="title is-2" style="text-align: center; padding-bottom: 10px;">Real Robot Experiments</h2>
          <div class="row">
            <div class="col">
              <video autoplay muted loop playsinline controls src="./static/videos/video_real_robot_experiments.mp4" width="100%" style="border-radius:5px; "></video>
            </div>
        </div>
      </div>
    </div>
  </div>
</section>

  


<!-- 
<section class="section">
  <div class="container is-max-desktop">
    <div class="content">
     <h2>Methodology and Evaluation</h2>
    </div>


    <div class="content" style="margin-top: 20px;">
      <ol>
        <li>An trajectory segmentation process based on the unsupervised learning method OPTICS (Ordering Points To Identify the Clustering Structure) is proposed and used to effectively segment different MPs from long human demonstrations.</li>
      
      <div style="text-align: center; margin-top: 20px; margin-bottom: 20px;">
        <img src="./static/images/3d.png"
            style="width: 90%; max-width: 1000px; height: auto; display: block; margin: 0 auto;"
      </div>
        <li>The transitional MPs are introduced to combine with different periodic MPs in a fluent way.</li>

      <div style="display: flex; justify-content: center; margin-top: 20px; margin-bottom: 20px;">
        <img src="./static/images/transitional.png" style="height: 280px; width: auto; max-width: 100%; object-fit: cover; margin: 0 0;" />
        <img src="./static/images/compare.png" style="height: 280px; width: auto; max-width: 100%; object-fit: cover; margin: 0 0;" />
      </div>

        <li>The proposed transitional MP method is evaluated to combine with periodic MPs in different phases, with different positions, velocities, and accelerations.</li>

      <div style="text-align: center; margin-top: 20px;"> 
        <img src="./static/images/3c.png"
            style="width: 90%; max-width: 1000px; height: auto; display: block; margin: 0 auto;"
      </div>
      </ol>
    </div>
  </div>
</section> 


<section class="section">
  <div class="container is-max-desktop">
    <div class="content has-text-justified">
      <h2>Real-World Experiments</h2>
      <p>We executed and estimate one of the typical multi-periodic movement skills, wiping the whiteboard to evaluate the proposed framework LAMPS. </p>
      
      <div style="display: flex; justify-content: center; margin-top: 20px; margin-bottom: 0; background-color: #f0f0f0; padding: 15px; border-radius: 8px;">
        <div style="text-align: center; margin: 0 5px;">
          <img src="./static/images/whiteboard.jpg" style="height: 220px; width: auto; max-width: 100%; object-fit: cover;" />
          <p>Human Skill of Wiping Whiteboard</p>
        </div>
        <div style="text-align: center; margin: 0 5px;">
          <img src="./static/images/scene.jpg" style="height: 220px; width: auto; max-width: 100%; object-fit: cover;" />
          <p>Experimental Scene diagram</p>
        </div>
        <div style="text-align: center; margin: 0 5px;">
          <img src="./static/images/res.png" style="height: 220px; width: auto; max-width: 100%; object-fit: cover;" />
          <p>Experimental Result Diagram</p>
        </div>
      </div>

      <div class="content has-text-justified" style="margin-top: 20px">
        <p> Here we show a clear overview of the experimental procedure using the propose framework. Please visit <strong><a href="./static/videos/video.mp4" target="_blank">here</a></strong> to see more details about our work experiment part.</p>
      </div>

      <div style="text-align: center; margin-top: 20px;">
        <img src="./static/images/exp.png"
             style="width: 90%; max-width: 1000px; height: auto; display: block; margin: 0 auto;"
      </div>
   </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="content">
      <h2>Example Results Analysis</h2>
      <p>Several periodic MPs and transitional MPs are contained in multi-periodic skills in human daily life. The following results show the fluent performance</p>
   
  <div style="display: flex; justify-content: center; margin-top: 20px; margin-bottom: 20px; background-color: #f0f0f0; padding: 15px; border-radius: 8px;">
    <div style="text-align: center; margin: 0 15px;">
      <img src="./static/videos/p.gif" style="height: 300px; width: auto; max-width: 100%; object-fit: cover;" />
      <p>Execution of First Periodic MP</p>
    </div>
    <div style="text-align: center; margin: 0 15px;">
      <img src="./static/videos/t.gif" style="height: 300px; width: auto; max-width: 100%; object-fit: cover;" />
      <p>Execution of First Transitional MP</p>
    </div>
  </div>
  </div>

  <p>And also, Impedance control can help to improve the performance of cleaning: </p>
  

<div class="columns is-centered" style="margin-top: 10px;">
<div class="column">
  <div class="content">
    <h4 class="title is-5" style="text-align: center;">With Impedance Control</h4>
    <video id="with" autoplay controls muted loop playsinline height="100%">
      <source src="./static/videos/with.mp4"
              type="video/mp4">
    </video>
  </div>
</div>

<div class="column">
  <h4 class="title is-5" style="text-align: center;">Without Impedance Control</h4>
  <div class="columns is-centered">
    <div class="column content">
      <video id="without" autoplay controls muted loop playsinline height="100%">
        <source src="./static/videos/without.mp4"
                type="video/mp4">
      </video>
    </div>
    </div>
    </div>
    </div>

   </div>
  </div>
</section>
-->

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX (Coming soon)</h2>
    <pre><code>@article{qin2026general,
                title={A General One-Shot Multimodal Active Perception Framework for Robotic Manipulation: Learning to Predict Optimal Viewpoint},
                author={Qin, Deyun and Liu, Zezhi and Luo, Hanqian and Liang, Xiao and Fang, Yongchun},
                journal={arXiv preprint arXiv:2601.13639},
                year={2026}
              }
    </code></pre>
<!--     <pre><code>@article{Darioush2024ControlBench,
  author    = {Darioush, Kevian and Usman, Syed and Xingang, Guo and Aaron, Havens and Geir, Dullerud and Peter, Seiler and Lianhui, Qin and Bin, Hu},
  title     = {Capabilities of Large Language Models in Control Engineering: A Benchmark Study on GPT-4, Claude 3 Opus, and Gemini 1.0 Ultra},
  journal   = {arXiv preprint arXiv:2404.03647},
  year      = {2024},
}</code></pre> -->
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a 
            <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This website was built using the
						<a href="https://nerfies.github.io/">Nerfies website</a> and the
						<a href="https://github.com/nerfies/nerfies.github.io">source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


<!-- <script>
  const sliderWrapper = document.querySelector('.slider-wrapper');
  const sliderItems = document.querySelectorAll('.slider-item');
  let currentIndex = 0;

  document.querySelector('.left-btn').addEventListener('click', () => {
    if (currentIndex > 0) {
      currentIndex--;
      updateSliderPosition();
    }
  });

  document.querySelector('.right-btn').addEventListener('click', () => {
    if (currentIndex < sliderItems.length - 1) {
      currentIndex++;
      updateSliderPosition();
    }
  });

  function updateSliderPosition() {
    const newTransformValue = -currentIndex * 100; // Assuming each slide is 100% of the container width
    sliderWrapper.style.transform = `translateX(${newTransformValue}%)`;
  }
</script> -->


<script>
  const sliderWrapper = document.querySelector('.slider-wrapper');
  const sliderItems = document.querySelectorAll('.slider-item');
  let currentIndex = 0;

  document.querySelector('.left-btn').addEventListener('click', () => {
    // Modify this to go to the last image if currently at the first image
    if (currentIndex > 0) {
      currentIndex--;
    } else {
      currentIndex = sliderItems.length - 1; // Go to the last image
    }
    updateSliderPosition();
  });

  document.querySelector('.right-btn').addEventListener('click', () => {
    // Reset currentIndex to 0 if on the last image, otherwise increment
    if (currentIndex < sliderItems.length - 1) {
      currentIndex++;
    } else {
      currentIndex = 0; // Reset to the first image
    }
    updateSliderPosition();
  });

  function updateSliderPosition() {
    const newTransformValue = -currentIndex * 100; // Assuming each slide is 100% of the container width
    sliderWrapper.style.transform = `translateX(${newTransformValue}%)`;
  }

  document.addEventListener('DOMContentLoaded', function () {
  const videos = [document.getElementById('with'), document.getElementById('without')];
  
  videos.forEach(video => {
    video.addEventListener('volumechange', function () {
      if (!video.muted) {
        video.muted = true;
      }
    });
  });
});

</script>


</body>
</html>
